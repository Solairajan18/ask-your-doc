# ğŸ“„ Ask Your Docs â€“ Simple RAG Chatbot

A **simple RAG-based â€œAsk Your Docsâ€ chatbot** built using **free and open-source tools only**.  
Users can upload documents and ask questions, and the chatbot answers **strictly from the uploaded documents** using Retrieval-Augmented Generation (RAG).

This project is intended as a **working demo**, not production software, and can be completed in **2â€“3 hours**.

---

## âœ¨ Features

- Single-page web UI (Streamlit)
- Upload and process multiple document formats:
  - PDF
  - Word (.docx)
  - Excel (.xlsx)
  - PowerPoint (.pptx)
  - TXT
  - JSON
- Ask natural-language questions about uploaded documents
- Answers generated **only from document content**
- Chat history displayed on the same page
- Runs fully **offline and locally**
- No paid APIs, no cloud services

---

## ğŸ§  Tech Stack

| Layer         | Tool                |
|--------------|---------------------|
| Frontend UI  | Streamlit           |
| Backend      | Python              |
| SLM (Local)  | Ollama              |
| Model        | Phi                 |
| Embeddings   | sentence-transformers|
| Embedding Model | all-MiniLM-L6-v2 |
| Embedding Dim | 384                |
| Vector DB    | FAISS (local)       |

---

## ğŸ—ï¸ Architecture (RAG Flow)

1. User uploads documents via the UI
2. Documents are parsed into raw text
3. Text is split into chunks
4. Each chunk is converted into an embedding
5. Embeddings are stored in FAISS
6. User question is embedded
7. FAISS retrieves the most relevant chunks
8. Retrieved context + question is sent to the local SLM
9. Answer is displayed in the UI

---

## ğŸ“‚ Project Structure

ask-your-doc/
â”‚
â”œâ”€â”€ app.py           # Streamlit UI and chat logic
â”œâ”€â”€ rag.py           # RAG pipeline (FAISS + Ollama)
â”œâ”€â”€ loaders.py       # Document parsing logic
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ data/
    â””â”€â”€ faiss_index/ # (Optional) persistent vector store

---

## ğŸš€ Setup Instructions

### 1ï¸âƒ£ Install Ollama

Download and install Ollama:

https://ollama.com/download

Pull the Phi model:
```bash
ollama pull phi
```

Test Ollama:
```bash
ollama run phi
```

### 2ï¸âƒ£ Create and Activate Virtual Environment
```bash
python -m venv venv
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
```

### 3ï¸âƒ£ Install Python Dependencies
```bash
pip install -r requirements.txt
```

### â–¶ï¸ Run the Application
```bash
streamlit run app.py
```

Open your browser:

http://localhost:8501

---

## âš™ï¸ Key Configuration Details

- Embedding model: all-MiniLM-L6-v2
- Embedding dimension: 384
- Chunk size: 300â€“500 words
- Top-K retrieval: 2â€“3 chunks
- Phi context window: ~2048 tokens

---

## âš ï¸ Common Mistakes to Avoid

- Forgetting to start Ollama before running the app
- Using the wrong FAISS embedding dimension
- Creating very large text chunks
- Expecting answers outside the uploaded documents
- Treating model warnings as errors (most are safe to ignore)

---

## âœ… Project Status

- Beginner-friendly
- Fully local execution
- Free-tier and open-source only
- Suitable for:
  - Learning RAG
  - Interview demos
  - GitHub portfolio projects

---


## âœ… Minimum Requirements (Working Demo)

| Component | Requirement |
|-----------|------------|
| OS        | Windows 10+, macOS, or Linux |
| CPU       | 4-core CPU (Intel i5 / Ryzen 5 or equivalent) |
| RAM       | 8 GB RAM |
| Disk Space| ~5â€“8 GB free |
| Python    | 3.9 â€“ 3.11 |
| Internet  | Required only for first-time model download |

---

## ğŸ“Œ Possible Enhancements (Optional)

- Show source chunks below each answer
- Persist FAISS index to disk
- Add document metadata and filtering
- Switch Phi â†’ Mistral or Phi-3
- Add multi-document comparison

---

## ğŸ“œ License

MIT License  
Free to use, modify, and share.

---

## ğŸ‘¤ Author

**Solai Rajan**  
[https://www.solairajan.space/](https://www.solairajan.space/)
